{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biological-basis",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SRCNNs.py import *\n",
    "from efficient_train.py import *\n",
    "from patch_transformations.py import *\n",
    "from predict.py import *\n",
    "from tranformation_func.py import *\n",
    "\n",
    "\n",
    "import scipy.misc\n",
    "import random\n",
    "import os\n",
    "from glob import glob\n",
    "import cv2\n",
    "from skimage.transform import pyramid_reduce, rotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interesting-material",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## generate training data ##############\n",
    "list_files_HR = glob(os.path.join('../Images/DIV2K_train_HR/', \"*.png\"))\n",
    "\n",
    "hr = []\n",
    "lr = []\n",
    "\n",
    "def train_crop(img, crop_factor):\n",
    "    r,c, _ = img.shape\n",
    "    rcrop, ccrop = int(r/crop_factor), int(c/crop_factor)\n",
    "    croped = []\n",
    "    for i in range(crop_factor):\n",
    "        for j in range(crop_factor):\n",
    "            croped.append(img[i*rcrop:(i+1)*rcrop,j*ccrop:(j+1)*ccrop,:])\n",
    "    return croped\n",
    "\n",
    "############ choose 2 from 800\n",
    "small_sample = random.sample(range(1,len(list_files_HR)), 2)\n",
    "\n",
    "version = 1\n",
    "for i in range(len(small_sample)): \n",
    "    k = small_sample[i]\n",
    "    file_HR = list_files_HR[k]\n",
    "    img_hr = cv2.imread(file_HR, cv2.IMREAD_COLOR)\n",
    "    img_hr = cv2.cvtColor(img_hr, cv2.COLOR_BGR2YCR_CB) #cv2.COLOR_BGR2RGB\n",
    "    target_size = (480, 480)  #(512, 512), 16, 2 #(512, 512), 8, 4\n",
    "    crop_factor = 10\n",
    "    scale = 4\n",
    "    shape = img_hr.shape[:2]\n",
    "    off_set_x = random.randint(0, int((shape[1] - target_size[1])/2))\n",
    "    off_set_y = random.randint(0, int((shape[0] - target_size[0])/2))\n",
    "    img_hr = img_hr[off_set_y:off_set_y + target_size[0], off_set_x:off_set_x + target_size[1]]\n",
    "    \n",
    "    # v1\n",
    "    if version == 1:\n",
    "        img_lr = cv2.resize(img_hr, (target_size[1] // (scale//2),\n",
    "                                    target_size[0] // (scale//2)), interpolation=cv2.INTER_LINEAR)\n",
    "        img_lr = cv2.GaussianBlur(img_lr,(5,5),0)\n",
    "        img_lr = cv2.resize(img_lr, (target_size[1] // scale,\n",
    "                                    target_size[0] // scale), interpolation=cv2.INTER_NEAREST)\n",
    "        img_lr = cv2.GaussianBlur(img_lr,(3,3),0)\n",
    "\n",
    "    # v2\n",
    "    if version == 2:\n",
    "        img_lr = cv2.resize(img_hr, (target_size[1] // (scale//2),\n",
    "                                    target_size[0] // (scale//2)), interpolation=cv2.INTER_LANCZOS4)\n",
    "        img_lr = cv2.resize(img_lr, (target_size[1] // scale,\n",
    "                                    target_size[0] // scale), interpolation=cv2.INTER_NEAREST)\n",
    "        img_lr = cv2.GaussianBlur(img_lr,(5,5),0)\n",
    "\n",
    "    # v3\n",
    "    if version == 3:\n",
    "        img_lr = pyramid_reduce(img_hr, downscale=4, sigma=None, order=1, mode='reflect', cval=0, multichannel=True)\n",
    "        img_lr = cv2.GaussianBlur(img_lr,(5,5),0)\n",
    "\n",
    "    # v4\n",
    "    if version == 4:\n",
    "        img_lr = pyramid_reduce(img_hr, downscale=2, sigma=None, order=1, mode='reflect', cval=0, multichannel=True)\n",
    "        img_lr = cv2.GaussianBlur(img_lr,(7,7),0)\n",
    "        img_lr = pyramid_reduce(img_lr, downscale=2, sigma=None, order=1, mode='constant', cval=0, multichannel=True)\n",
    "        img_lr = cv2.GaussianBlur(img_lr,(3,3),0)\n",
    "    \n",
    "    hr.extend(train_crop(img_hr, crop_factor))\n",
    "    lr.extend(train_crop(img_lr, crop_factor))\n",
    "    \n",
    "idx = np.random.randint(len(hr), size=250) ##### randomly draw patches\n",
    "hr = np.asarray(hr)[idx,:,:,:]\n",
    "lr = np.asarray(lr)[idx,:,:,:]\n",
    "    \n",
    "print(len(hr), hr[0].shape)\n",
    "print(len(lr), lr[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structured-wednesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### processing raw data\n",
    "x = hr\n",
    "y = lr\n",
    "for i in range(len(x_data)):\n",
    "    x_90 = cv2.rotate(x_data[i], cv2.ROTATE_90_CLOCKWISE)\n",
    "    x_180 = cv2.rotate(x_data[i], cv2.ROTATE_180)\n",
    "    x_270 = cv2.rotate(x_data[i], cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "    y_90 = cv2.rotate(y_data[i], cv2.ROTATE_90_CLOCKWISE)\n",
    "    y_180 = cv2.rotate(y_data[i], cv2.ROTATE_180)\n",
    "    y_270 = cv2.rotate(y_data[i], cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "    x.extend((x_data[i], x_90, x_180, x_270))\n",
    "    y.extend((y_data[i], y_90, y_180, y_270))\n",
    "\n",
    "Yx_data= np.asarray(x)[:,:,:,0]\n",
    "Yy_data= np.asarray(y)[:,:,:,0]\n",
    "\n",
    "input_r, input_c = Yx_data.shape[1], Yx_data.shape[2]\n",
    "\n",
    "input_shape_dct, output_shape_dct, x_train_dct, y_train_dct = process_data(Yx_data,Yy_data,space) \n",
    "input_shape_db6, output_shape_db6, x_train_db6, y_train_db6 = process_data_wave(Yx_data,Yy_data,db6)\n",
    "\n",
    "padding_shape = np.max([input_shape_db6[0], input_shape_dct[0]])\n",
    "train_unpad = [x_train_dct, x_train_db6]\n",
    "train = []\n",
    "## pad to the same shape\n",
    "for i in range(len(train_unpad)):\n",
    "    if train_unpad[i].shape[1] == padding_shape:\n",
    "        train.append(train_unpad[i])\n",
    "    else:\n",
    "        target = train_unpad[i]\n",
    "        v = []\n",
    "        for j in range(target.shape[0]):\n",
    "            temp = padding(target[j, :,:,:], padding_shape)\n",
    "            v.append(temp)\n",
    "        train.append(np.asarray(v))\n",
    "        \n",
    "## fold to one tensor\n",
    "for i in range(1,len(train)):\n",
    "    train[0] = np.concatenate((train[0], train[i]), axis=3)\n",
    "x_train = train[0]\n",
    "y_train = np.reshape(Yy_data, (Yy_data.shape[0],Yy_data.shape[1],Yy_data.shape[2],1))\n",
    "input_shape = x_train.shape[1:]\n",
    "output_shape = y_train.shape[1:]\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chinese-government",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Input(shape = input_shape)\n",
    "models = Model(input_img, frame_dct(input_img, output_shape))\n",
    "models.compile(loss= 'mean_squared_error', optimizer = 'adam')\n",
    "models.summary()\n",
    "\n",
    "epoch_count = 0       # initial epoch setting\n",
    "lower_bound = 10e4    # threshold of explosion, above this value, read and retrain\n",
    "psnr_value = 0        # initial psnr value setting\n",
    "marker = 500          # max loss when trying testing psnr value\n",
    "filename = 'edsr_resnet.h5'\n",
    "max_epoch = 1000      # max number of epoch to train\n",
    "lower_bound_min = 50  # hard stop loss value of training\n",
    "####models = load_model(filename,custom_objects={'tf':tf})\n",
    "\n",
    "efficienct_train(models, x_train, y_train, epoch_count, lower_bound,marker, psnr_value,filename, max_epoch, lower_bound_min)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
